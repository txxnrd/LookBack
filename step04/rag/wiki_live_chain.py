# rag/wiki_live_chain.py
from langchain_community.document_loaders import WikipediaLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline


def build_wiki_live_chain(
    model_name: str,
    device: int | str = 0,
    *,
    return_retriever_only: bool = False,   # â† ìƒˆ ì˜µì…˜
):
    """Wikipedia ì‹¤ì‹œê°„ ê²€ìƒ‰ + LLM íŒŒì´í”„ë¼ì¸ ìƒì„±

    Parameters
    ----------
    model_name : str
        ğŸ¤— í—ˆë¸Œ ëª¨ë¸ ì´ë¦„ (ex. 'meta-llama/Llama-7b-chat-hf')
    device : int | str
        0, 1 â€¦ ë˜ëŠ” 'cuda:0' ì‹ ID
    return_retriever_only : bool, default False
        â€¢ False â†’ ê¸°ì¡´ê³¼ ë™ì¼: RetrievalQA ì²´ì¸ ë¦¬í„´  
        â€¢ True  â†’ (retriever, tokenizer) íŠœí”Œë§Œ ë¦¬í„´
                  â†’ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì“°ê³  ì‹¶ì„ ë•Œ í¸ë¦¬
    """
    # 1) LLM (generate ìš©)
    llm = HuggingFacePipeline(model_name=model_name, device=device)

    # 2) ë¬¸ë‹¨ ì„ë² ë”© ëª¨ë¸
    emb_model = HuggingFaceEmbeddings(model_name="intfloat/e5-base-v2")

    # 3) on-the-fly ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ í•¨ìˆ˜
    def wiki_retriever(query: str):
        # (1) ìœ„í‚¤ ë¬¸ë‹¨ 3ê°œ ë¡œë“œ
        docs = WikipediaLoader(
            query=query,
            lang="en",
            load_max_docs=3,
        ).load_and_split()

        # (2) ë¬¸ë‹¨ ìˆ˜ê°€ ì‘ìœ¼ë‹ˆ ì¦‰ì„ ë²¡í„° DB ìƒì„± â†’ top-k=4 ë°˜í™˜
        store = FAISS.from_documents(docs, emb_model)
        return store.similarity_search(query, k=4)

    # 4) RetrievalQA í”„ë¡¬í”„íŠ¸ (ì²´ì¸ì„ ê·¸ëŒ€ë¡œ ì“¸ ê²½ìš°)
    prompt = PromptTemplate(
        template="{context}\n\n#Question#: {question}\n#Answer#:",
        input_variables=["context", "question"],
    )

    # 5) ì²´ì¸ êµ¬ì„±
    qa_chain = RetrievalQA(
        llm=llm,
        retriever=wiki_retriever,
        prompt=prompt,
    )

    # 6) ì˜µì…˜ì— ë”°ë¼ ë°˜í™˜
    if return_retriever_only:
        return qa_chain.retriever, llm.tokenizer
    return qa_chain
